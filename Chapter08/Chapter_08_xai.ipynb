{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7f9744",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Building-Agentic-AI-Systems/blob/main/Chapter_08.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Chapter 08 - Explainable AI (XAI) techniques \n",
    "\n",
    "Example of how attention visualization, saliency maps, and natural language explanations can be generated simply using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for model, tokenizer, and visualization\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e56c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # Specify pre-trained model name\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)  # Load tokenizer for model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Load BERT model for sequence classification with 2 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da15a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text\n",
    "travel_query = \"What are the best family-friendly travel destinations in Europe?\"  # Example travel-related query\n",
    "inputs = tokenizer(travel_query, return_tensors=\"pt\", truncation=True, padding=True)  # Tokenize input query with padding and truncation\n",
    "input_ids = inputs['input_ids']  # Extract token IDs for the input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get attention scores from the model\n",
    "def get_attention_scores(model, inputs):\n",
    "    # Run model with input and extract attention scores\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    return outputs.attentions  # Return attention scores from all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize attention scores\n",
    "def visualize_attention(attention_scores, tokens):\n",
    "    sns.set(style='whitegrid')  # Set seaborn style for heatmap\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))  # Create a figure for plotting\n",
    "    attention_layer = attention_scores[-1][0]  # Extract the last attention layer's scores\n",
    "    attention_weights = attention_layer[0].detach().numpy()  # Convert attention scores to numpy array for plotting\n",
    "\n",
    "    # Plot the attention heatmap\n",
    "    sns.heatmap(attention_weights, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", ax=ax)\n",
    "    plt.title(\"Attention Visualization\")  # Title for the plot\n",
    "    plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention scores from the model\n",
    "attention_scores = get_attention_scores(model, inputs)\n",
    "\n",
    "# Decode the input IDs to get human-readable tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# Visualize the attention scores for the tokens in the input\n",
    "visualize_attention(attention_scores, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238c484",
   "metadata": {},
   "source": [
    "## Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library for saliency map visualization\n",
    "from captum.attr import Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf893cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saliency Map Visualization function to visualize the importance of each token\n",
    "def visualize_saliency(sentence):\n",
    "    # Step 1: Tokenize the input sentence\n",
    "    # Use the BERT tokenizer to convert the input sentence into token IDs and attention masks.\n",
    "    # Attention mask ensures the padding tokens are ignored during processing.\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Step 2: Ensure the model is in evaluation mode\n",
    "    # In evaluation mode, the model will not perform backpropagation (no parameter updates),\n",
    "    # which is important for inference and visualizations.\n",
    "    model.eval()\n",
    "    \n",
    "    # Step 3: Get embeddings and enable gradient tracking\n",
    "    # Get the token embeddings from the model's input embedding layer and enable gradient tracking.\n",
    "    # This allows the saliency map to be computed based on the gradients of the embeddings.\n",
    "    embeddings = model.get_input_embeddings()(input_ids).requires_grad_()\n",
    "\n",
    "    # Step 4: Define a custom forward function for the model\n",
    "    # The forward function will process the embeddings directly and return the model's logits (predictions).\n",
    "    # We bypass the usual model inputs to directly feed the embeddings for saliency calculation.\n",
    "    def forward_with_logits(embeddings):\n",
    "        return model(inputs_embeds=embeddings, attention_mask=attention_mask).logits\n",
    "\n",
    "    # Step 5: Initialize Saliency and compute the saliency scores\n",
    "    # Saliency scores measure the contribution of each token to the model's prediction.\n",
    "    saliency = Saliency(forward_with_logits)\n",
    "    \n",
    "    # Get the saliency scores for the input embeddings with respect to the positive class (target=1)\n",
    "    saliency_scores = saliency.attribute(embeddings, target=1)\n",
    "\n",
    "    # Step 6: Convert token IDs back to human-readable tokens\n",
    "    # This step helps us map the token IDs back into actual words for better interpretability.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Step 7: Aggregate the saliency scores for visualization\n",
    "    # Summing the saliency scores across the token's embedding dimension to get a single score for each token.\n",
    "    # This helps in visualizing the importance of each token in a 2D plot.\n",
    "    saliency_scores = saliency_scores.sum(dim=2).squeeze()\n",
    "\n",
    "    # Step 8: Visualize the saliency map\n",
    "    # Create a bar chart where each token's importance score is plotted, allowing us to see which tokens\n",
    "    # are most important for the model's decision. Tokens on the x-axis and saliency scores on the y-axis.\n",
    "    plt.bar(range(len(tokens)), saliency_scores.detach().numpy(), tick_label=tokens, color='teal')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability of tokens\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add a title to the plot\n",
    "    plt.title(\"Saliency Map\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13084492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input sentence\n",
    "travel_query = \"What are the best family-friendly travel destinations in Europe?\"\n",
    "\n",
    "# Example sentence to visualize saliency for\n",
    "visualize_saliency(travel_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549796e",
   "metadata": {},
   "source": [
    "## Natural Language Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ebf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an explanation using OpenAI\n",
    "def generate_explanation_with_openai(text):\n",
    "    # Initialize the OpenAI client with the API key stored in the environment variables\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    # Use the OpenAI client to make a request for generating a completion (explanation)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to be used for generation\n",
    "        messages=[  # Provide the context for the model\n",
    "            {\"role\": \"system\", \"content\": \"You are an explainability assistant.\"},  # System message setting the assistant's role\n",
    "            {\"role\": \"user\", \"content\": f\"Explain why '{text}' is important in the context of travel.\"}  # User query with the input text\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the generated explanation from the response object\n",
    "    explanation = response.choices[0].message.content\n",
    "\n",
    "    # Return the explanation text\n",
    "    return explanation\n",
    "\n",
    "# Example usage: calling the function with a sample input\n",
    "explanation = generate_explanation_with_openai(travel_query)\n",
    "\n",
    "# Output the generated explanation\n",
    "print(f\"Explanation:\\n{explanation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c972cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
