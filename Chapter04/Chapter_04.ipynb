{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ef500a",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Building-Agentic-AI-Systems/blob/main/Chapter_04.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Chapter 4 â€“ Reflection and Introspection in Agents\n",
    "---\n",
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai ipywidgets crewai pysqlite3-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478113ec",
   "metadata": {},
   "source": [
    "# 1. Meta Reasoning - example\n",
    "---\n",
    "\n",
    "Let's take a look at a simple meta-reasoning approach without AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2139",
   "metadata": {},
   "source": [
    "## Simulated travel agent with meta-reasoning capabilities\n",
    "\n",
    "- recommend_destination: The agent recommends a destination based on user preferences (budget, luxury, adventure) and internal weightings.\n",
    "\n",
    "- get_user_feedback: The agent receives feedback on the recommendation (positive or negative).\n",
    "\n",
    "- meta_reasoning: The agent adjusts its reasoning by updating the weights based on feedback, improving future recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated travel agent with meta-reasoning capabilities\n",
    "class ReflectiveTravelAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize preference weights that determine how user preferences influence recommendations\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.5,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.2  # Weight for adventure-related preferences\n",
    "        }\n",
    "        self.user_feedback = []  # List to store user feedback for meta-reasoning\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences with keys like 'budget', 'luxury', 'adventure'\n",
    "\n",
    "        Returns:\n",
    "            str: Recommended destination\n",
    "        \"\"\"\n",
    "        # Calculate scores for each destination based on weighted user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        # Select the destination with the highest calculated score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        return recommendation\n",
    "\n",
    "    def get_user_feedback(self, actual_experience):\n",
    "        \"\"\"\n",
    "        Simulate receiving user feedback and trigger meta-reasoning to adjust recommendations.\n",
    "\n",
    "        Args:\n",
    "            actual_experience (str): The destination the user experienced\n",
    "        \"\"\"\n",
    "        # Simulate user feedback: 1 for positive, -1 for negative\n",
    "        feedback = random.choice([1, -1])\n",
    "        print(f\"Feedback for {actual_experience}: {'Positive' if feedback == 1 else 'Negative'}\")\n",
    "        \n",
    "        # Store the feedback for later analysis\n",
    "        self.user_feedback.append((actual_experience, feedback))\n",
    "        \n",
    "        # Trigger meta-reasoning to adjust the agent's reasoning process based on feedback\n",
    "        self.meta_reasoning()\n",
    "\n",
    "    def meta_reasoning(self):\n",
    "        \"\"\"\n",
    "        Analyze collected feedback and adjust preference weights to improve future recommendations.\n",
    "        This simulates the agent reflecting on its reasoning process and making adjustments.\n",
    "        \"\"\"\n",
    "        for destination, feedback in self.user_feedback:\n",
    "            if feedback == -1:  # Negative feedback indicates dissatisfaction\n",
    "                # Reduce the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 0.9  # Decrease luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "            elif feedback == 1:  # Positive feedback indicates satisfaction\n",
    "                # Increase the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 1.1  # Increase luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "\n",
    "        # Normalize weights to ensure they sum up to 1 for consistency\n",
    "        total_weight = sum(self.preferences_weights.values())\n",
    "        for key in self.preferences_weights:\n",
    "            self.preferences_weights[key] /= total_weight\n",
    "\n",
    "        # Display updated weights after meta-reasoning adjustments\n",
    "        print(f\"Updated weights: {self.preferences_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad2f0",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "- User Preferences: Defines the user's preferences for budget, luxury, and adventure.\n",
    "\n",
    "- First Recommendation: The agent recommends a destination based on the initial weights and user preferences.\n",
    "\n",
    "- User Feedback Simulation: Simulates the user providing feedback on the recommended destination.\n",
    "\n",
    "- Second Recommendation: After adjusting the weights based on feedback, the agent makes a new recommendation that reflects the updated reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ReflectiveTravelAgent()\n",
    "\n",
    "    # User's initial preferences\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.8,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.5    # Moderate preference for adventure activities\n",
    "    }\n",
    "\n",
    "    # First recommendation based on initial preferences and weights\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Recommended destination: {recommended}\")\n",
    "\n",
    "    # Simulate user experience and provide feedback\n",
    "    agent.get_user_feedback(recommended)\n",
    "\n",
    "    # Second recommendation after adjusting weights based on feedback\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Updated recommendation: {recommended}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cb609",
   "metadata": {},
   "source": [
    "## Meta-reasoning with AI\n",
    "---\n",
    "\n",
    "Now let's bring in AI to perform meta-reasoning with agents. In this case we will use CrewAI framework to create our meta-reasoning Agents with OpenAI LLMs. We will also emulate a user feedback using AI just for demonstration purposes. First, let's make sure we initialize our OpenAI API key and then let's define the \"Crew\" (with CrewAI) and the Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69487942",
   "metadata": {},
   "source": [
    "We will define three tools that our agents will use-\n",
    "\n",
    "1. `recommend_destination`: This tool will use a set of base weights that prioritizes budget, luxury, and adventure equally and then uses user's preference weights to recommend a destination. Paris will emphasize luxury, NYC emphasizes luxury and adventure, whereas Bangkok emphasizes budget.\n",
    "2. `update_weights_on_feedback`: This tool will update the internal base weights based on the user's feedback on the recommended destination. A positive feedback will tell the model that it's recommendation is correct and it needs to update it's internal base weights based and increase it by a given (arbitrary adjustment factor), or reduce the weights using the adjustment factor if the feedback is dissatisfied.\n",
    "3. `feedback_emulator`: This tool will emulate a user prividing \"satisfied\" or \"dissatisfied\" feedback to the AI agent's destination recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "# Tool 1\n",
    "@tool(\"Recommend travel destination based on preferences.\")\n",
    "def recommend_destination(user_preferences: dict) -> str:\n",
    "    \"\"\"\n",
    "    Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "    Args:\n",
    "        user_preferences (dict): User's preferences with keys - 'budget', 'luxury', 'adventure'\n",
    "                                default user_preference weights 'budget' = 0.8, 'luxury' = 0.2, 'adventure' = 0.5\n",
    "                                user_preferences = {\n",
    "                                                \"budget\": 0.8,\n",
    "                                                \"luxury\": 0.4,\n",
    "                                                \"adventure\": 0.3\n",
    "                                            }\n",
    "    Returns:\n",
    "        str: Recommended destination\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "            \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "        }\n",
    "   # Calculate weighted scores for each destination\n",
    "    score = {\n",
    "        \"Paris\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +      # Paris emphasizes luxury\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        ),\n",
    "        \"Bangkok\": (\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"] * 2 +  # Bangkok emphasizes budget\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"]\n",
    "        ),\n",
    "        \"New York\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] * 1.5 +  # NYC emphasizes luxury and adventure\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] * 1.5 +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Select the destination with the highest calculated score\n",
    "    recommendation = max(score, key=score.get)\n",
    "    return recommendation\n",
    "\n",
    "# Tool 2\n",
    "@tool(\"Reasoning tool to adjust preference weights based on user feedback.\")\n",
    "def update_weights_on_feedback(destination: str, feedback: int, adjustment_factor: float) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze collected feedback and adjust internal preference weights based on user feedback for better future recommendations.\n",
    "\n",
    "    Args:        \n",
    "        destination (str): The destination recommended ('New York', 'Bangkok' or 'Paris')\n",
    "        feedback (int): Feedback score; 1 = Satisfied, -1 = dissatisfied\n",
    "        adjustment_factor (int): The adjustment factor between 0 and 1 that will be used to adjust the internal weights.\n",
    "                                 Value will be used as (1 - adjustment_factor) for dissatisfied feedback and (1 + adjustment_factor)\n",
    "                                 for satisfied feedback.\n",
    "    Returns:\n",
    "        dict: Adjusted internal weights\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "        \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "        \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "        \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "    }\n",
    "\n",
    "    # Define primary and secondary characteristics for each destination\n",
    "    destination_characteristics = {\n",
    "        \"Paris\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"Bangkok\": {\n",
    "            \"primary\": \"budget\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"New York\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Get the characteristics for the given destination\n",
    "    dest_chars = destination_characteristics.get(destination, {})\n",
    "    primary_feature = dest_chars.get(\"primary\")\n",
    "    secondary_feature = dest_chars.get(\"secondary\")\n",
    "\n",
    "    # adjustment_factor = 0.2  # How much to adjust weights by\n",
    "\n",
    "    if feedback == -1:  # Negative feedback\n",
    "        # Decrease weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 - adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 - adjustment_factor/2)\n",
    "            \n",
    "    elif feedback == 1:  # Positive feedback\n",
    "        # Increase weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 + adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 + adjustment_factor/2)\n",
    "\n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    total_weight = sum(internal_default_weights.values())\n",
    "    for key in internal_default_weights:\n",
    "        internal_default_weights[key] = round(internal_default_weights[key] / total_weight, 2)\n",
    "\n",
    "    # Ensure weights sum to exactly 1.0 after rounding\n",
    "    adjustment = 1.0 - sum(internal_default_weights.values())\n",
    "    if adjustment != 0:\n",
    "        # Add any rounding difference to the largest weight\n",
    "        max_key = max(internal_default_weights, key=internal_default_weights.get)\n",
    "        internal_default_weights[max_key] = round(internal_default_weights[max_key] + adjustment, 2)\n",
    "\n",
    "    return internal_default_weights\n",
    "\n",
    "# Tool 3\n",
    "@tool(\"User feedback emulator tool\")\n",
    "def feedback_emulator(destination: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a destination recommendation (such as 'New York' or 'Bangkok') this tool will emulate to provide\n",
    "    a user feedback as 1 (satisfied) or -1 (dissatisfied)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    feedback = random.choice([-1, 1])\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe973f1c",
   "metadata": {},
   "source": [
    "Once, the tools are defined, we will declare three CrewAI Agents each of which will use one of the tools above. The `meta_agent` is basically the agent that will perform meta-reasoning using the emulated user feedback and the previously recommended destination to update the internal weights using an `adjustment_factor`. \n",
    "\n",
    "Note that here, the model assigns an adjustment factor dynamically to adjust the internal system weights (which is `{\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33}` in the beginning), i.e. we are not hard coding the adjustment factor. Although, the nature of user feedback in this example is limited to \"satisfied\" or \"dissatisfied\" (1 or -1), feedback can be of various forms and may contain more details, in which case your AI Agent may adjust different values to the adjustment_factor. More contextual feedback with details will help the model perform better meta-reasoning on it's previous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from typing import Dict, Union\n",
    "import random\n",
    "\n",
    "# Utility functions\n",
    "def process_recommendation_output(output: str) -> str:\n",
    "    \"\"\"Extract the clean destination string from the agent's output.\"\"\"\n",
    "    # Handle various ways the agent might format the destination\n",
    "    for city in [\"Paris\", \"Bangkok\", \"New York\"]:\n",
    "        if city.lower() in output.lower():\n",
    "            return city\n",
    "    return output.strip()\n",
    "\n",
    "def process_feedback_output(output: Union[Dict, str]) -> int:\n",
    "    \"\"\"Extract the feedback value from the agent's output.\"\"\"\n",
    "    if isinstance(output, dict):\n",
    "        return output.get('feedback', 0)\n",
    "    try:\n",
    "        # Try to parse as integer if it's a string\n",
    "        return int(output)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def generate_random_preferences():\n",
    "    # Generate 3 random numbers and normalize them\n",
    "    values = [random.random() for _ in range(3)]\n",
    "    total = sum(values)\n",
    "    \n",
    "    return {\n",
    "        \"budget\": round(values[0]/total, 2),\n",
    "        \"luxury\": round(values[1]/total, 2),\n",
    "        \"adventure\": round(values[2]/total, 2)\n",
    "    }\n",
    "\n",
    "# Initial shared state for weights, preferences, and results\n",
    "state = {\n",
    "    \"weights\": {\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33},\n",
    "    \"preferences\": generate_random_preferences()\n",
    "}\n",
    "\n",
    "# Agents\n",
    "preference_agent = Agent(\n",
    "    name=\"Preference Agent\",\n",
    "    role=\"Travel destination recommender\",\n",
    "    goal=\"Provide the best travel destination based on user preferences and weights.\",\n",
    "    backstory=\"An AI travel expert adept at understanding user preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[recommend_destination]\n",
    ")\n",
    "\n",
    "feedback_agent = Agent(\n",
    "    name=\"Feedback Agent\",\n",
    "    role=\"Simulated feedback provider\",\n",
    "    goal=\"Provide simulated feedback for the recommended travel destination.\",\n",
    "    backstory=\"An AI that mimics user satisfaction or dissatisfaction for travel recommendations.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[feedback_emulator]\n",
    ")\n",
    "\n",
    "meta_agent = Agent(\n",
    "    name=\"Meta-Reasoning Agent\",\n",
    "    role=\"Preference weight adjuster\",\n",
    "    goal=\"Reflect on feedback and adjust the preference weights to improve future recommendations.\",\n",
    "    backstory=\"An AI optimizer that learns from user experiences to fine-tune recommendation preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[update_weights_on_feedback]\n",
    ")\n",
    "\n",
    "\n",
    "# Tasks with data passing\n",
    "generate_recommendation = Task(\n",
    "    name=\"Generate Recommendation\",\n",
    "    agent=preference_agent,\n",
    "    description=(\n",
    "        f\"Use the recommend_destination tool with these preferences: {state['preferences']}\\n\"\n",
    "        \"Return only the destination name as a simple string (Paris, Bangkok, or New York).\"\n",
    "    ),\n",
    "    expected_output=\"A destination name as a string\",\n",
    "    output_handler=process_recommendation_output\n",
    ")\n",
    "\n",
    "simulate_feedback = Task(\n",
    "    name=\"Simulate User Feedback\",\n",
    "    agent=feedback_agent,\n",
    "    description=(\n",
    "        \"Use the feedback_emulator tool with the destination from the previous task.\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Get the destination string from the previous task\\n\"\n",
    "        \"2. Pass it directly to the feedback_emulator tool\\n\"\n",
    "        \"3. Return the feedback value (1 or -1)\\n\\n\"\n",
    "        \"IMPORTANT: Pass the destination as a plain string, not a dictionary.\"\n",
    "    ),\n",
    "    expected_output=\"An integer feedback value: 1 or -1\",\n",
    "    context=[generate_recommendation],\n",
    "    output_handler=process_feedback_output\n",
    ")\n",
    "\n",
    "adjust_weights = Task(\n",
    "    name=\"Adjust Weights Based on Feedback\",\n",
    "    agent=meta_agent,\n",
    "    description=(\n",
    "        \"Use the update_weights_on_feedback tool with:\\n\"\n",
    "        \"1. destination: Get from first task's output (context[0])\\n\"\n",
    "        \"2. feedback: Get from second task's output (context[1])\\n\"\n",
    "        \"3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\\n\\n\"\n",
    "        \"Ensure all inputs are in their correct types (string for destination, integer for feedback).\"\n",
    "    ),\n",
    "    expected_output=\"Updated weights as a dictionary\",\n",
    "    context=[generate_recommendation, simulate_feedback]\n",
    ")\n",
    "\n",
    "# Crew Definition\n",
    "crew = Crew(\n",
    "    agents=[preference_agent, feedback_agent, meta_agent],\n",
    "    tasks=[generate_recommendation, simulate_feedback, adjust_weights],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "result = crew.kickoff()\n",
    "print(\"\\nFinal Results:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104c35a",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Self Explanation - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd7e0",
   "metadata": {},
   "source": [
    "\n",
    "In this section we will see how to implement transparency, learning and refinement, user engagement & collaboration.\n",
    "\n",
    "1. **Self-Explanation transparency**: For each recommendation, the agent generates a detailed self-explanation. This explanation outlines the factors that led to the recommendation, such as proximity to popular attractions, budget-friendly options, or the presence of adventure activities. The purpose is to provide transparency into how the decision was made, helping the user understand the reasoning process.\n",
    "\n",
    "2. **Learning and refinement**: The agent doesn't stop after making the recommendation. It actively reflects on user feedback (whether positive or negative). If the feedback is negative, it introspects on its decision-making process and adjusts the importance (weights) it assigns to user preferences for future recommendations. For instance, if a user dislikes a budget-friendly recommendation, the agent might reduce the emphasis it places on budget-related preferences.\n",
    "\n",
    "3. **User Engagement**: The class also simulates a dialogue with the user. After giving the recommendation and the self-explanation, it collects feedback from the user, allowing for a more collaborative interaction. This feedback is then used to refine future recommendations, making the agent more adaptive and personalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f241f2",
   "metadata": {},
   "source": [
    "### 2.1 Transparency: Verbalizing Reasoning in Decisions\n",
    "\n",
    "Lets use OpenAI SDK to see how a model can perform reasoning in the decisions it makes. Here, the agent generates explanations for its reasoning when recommending a travel itinerary. It uses GPT-4o-mini to generate self-explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Mock data for the travel recommendation\n",
    "user_preferences = {\n",
    "    \"location\": \"Paris\",\n",
    "    \"budget\": 200,\n",
    "    \"preferences\": [\"proximity to attractions\", \"user ratings\"],\n",
    "}\n",
    "\n",
    "# Input reasoning factors for the GPT model\n",
    "reasoning_prompt = f\"\"\"\n",
    "You are an AI-powered travel assistant. Explain your reasoning behind a hotel recommendation for a user traveling to {user_preferences['location']}.\n",
    "Consider:\n",
    "1. Proximity to popular attractions.\n",
    "2. High ratings from similar travelers.\n",
    "3. Competitive pricing within ${user_preferences['budget']} budget.\n",
    "4. Preferences: {user_preferences['preferences']}.\n",
    "Provide a clear, transparent self-explanation.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a reflective travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print self-explanation\n",
    "print(\"Agent Self-Explanation:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8b23e",
   "metadata": {},
   "source": [
    "#### Using Crew AI\n",
    "\n",
    "Our previous example was pretty simple and didn't use Agents. But with an agentic system you may have agents actually lookup hotels appropriate to the user query using tools. Subsequently, a second agent may perform the self-explanation transparency on the response. Let's first define a tool that will respond back with mock hotel data based on price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "# Tool 1\n",
    "@tool(\"Recommend hotels based on user query.\")\n",
    "def recommend_hotel(cost_per_night: int) -> str:\n",
    "    \"\"\"\n",
    "    Returns hotels based on cost per night.\n",
    "\n",
    "    Args:\n",
    "        cost_per_night (int): User's preference of hotel room per night cost. \n",
    "            \n",
    "    \"\"\"\n",
    "    static_hotels = [\n",
    "        {\n",
    "            \"hotel_name\": \"Le Royal Monceau Raffles\",\n",
    "            \"price_per_night\": 1200,\n",
    "            \"transportation_convenience\": \"convenient\",\n",
    "            \"location\": \"8th arrondissement\",\n",
    "            \"nearest_metro\": \"Charles de Gaulle-Ã‰toile\",\n",
    "            \"distance_from_metro\": '1 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Citadines Les Halles\",\n",
    "            \"price_per_night\": 250,\n",
    "            \"transportation_convenience\": \"convenient\",\n",
    "            \"location\": \"1st arrondissement\",\n",
    "            \"nearest_metro\": \"Les Halles\",\n",
    "            \"distance_from_metro\": '2.8 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Ibis Paris Montmartre\",\n",
    "            \"price_per_night\": 120,\n",
    "            \"transportation_convenience\": \"moderate\",\n",
    "            \"location\": \"18th arrondissement\",\n",
    "            \"nearest_metro\": \"Place de Clichy\",\n",
    "            \"distance_from_metro\": '5 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Four Seasons Hotel George V\",\n",
    "            \"price_per_night\": 1500,\n",
    "            \"transportation_convenience\": \"convenient\",\n",
    "            \"location\": \"8th arrondissement\",\n",
    "            \"nearest_metro\": \"George V\",\n",
    "            \"distance_from_metro\": '1 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Hotel du Petit Moulin\",\n",
    "            \"price_per_night\": 300,\n",
    "            \"transportation_convenience\": \"moderate\",\n",
    "            \"location\": \"3rd arrondissement\",\n",
    "            \"nearest_metro\": \"Saint-SÃ©bastien Froissart\",\n",
    "            \"distance_from_metro\": '1.9 km'\n",
    "        }\n",
    "    ]\n",
    "    matching_hotels = [\n",
    "        hotel for hotel in static_hotels \n",
    "        if cost_per_night <= hotel[\"price_per_night\"]\n",
    "    ]\n",
    "    return matching_hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66a8c9",
   "metadata": {},
   "source": [
    "Now we will perform the same transparency reasoning with a CrewAI Agent/Task combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "travel_agent = Agent(\n",
    "    role=\"Travel Advisor\",\n",
    "    goal=\"Provide hotel recommendations with transparent reasoning.\",\n",
    "    backstory=\"\"\"\n",
    "    An AI travel advisor specializing in personalized travel planning. \n",
    "    You always explain the steps you take to arrive at a conclusion\n",
    "    \"\"\",\n",
    "    allow_delegation=False,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[recommend_hotel]\n",
    ")\n",
    "\n",
    "recommendation_task = Task(\n",
    "    name=\"Recommend hotel\",\n",
    "    description=\"\"\"\n",
    "    Recommend a hotel based on the user's query: \n",
    "    '{query}'.\n",
    "    \"\"\",\n",
    "    agent=travel_agent,\n",
    "    expected_output=\"The hotel recommendation and reasoning in the following format\\n\\nHotel: [Your answer]\\n\\nPrice/night: [The price]\\n\\nReason: [Detailed breakdown of your thought process]\"\n",
    ")\n",
    "\n",
    "travel_crew = Crew(\n",
    "    agents=[travel_agent],\n",
    "    tasks=[recommendation_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "travel_crew.kickoff(inputs={'query': \"I am looking for a hotel in Paris under $300 a night.\"})\n",
    "\n",
    "# Retrieve and print the output\n",
    "output = recommendation_task.output\n",
    "print(\"Hotel Recommendation and Explanation:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1c4b0",
   "metadata": {},
   "source": [
    "Not only does our Agent can lookup hotels using the tool but it clearly explains why it gave the recommendation as `Reason`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd239c6",
   "metadata": {},
   "source": [
    "### 2.2 Learning and Refinement: Using Self-Explanation to Identify Gaps\n",
    "\n",
    "While our self-explaining agent is great, the user may still not like the recommendation it gave. In which case the user may express their dissatisfaction with the recommendation. This is where we need learning and refinement of the approach. In our case we may extend the previous agent based system to now also include a second agent that can take user feedback and re-strategize on its approach to recommend a hotel. Note that in this case, sequential execution or parallel execution of the agents may not be appropriate, thus we need a hierarchical approach where a top level agent can manage the two agents and then delegate tasks accordingly.\n",
    "\n",
    "Lets define our learning and refinement agent that can take user feedback and it's previous recommendation in context and then complete the task by refining it's strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc669bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "reflective_travel_agent = Agent(\n",
    "    role=\"Self-Improving Travel Advisor\",\n",
    "    goal=\"Refine hotel recommendations based on user feedback to your previous recommendation to improve decision-making.\",\n",
    "    backstory=\"\"\"\n",
    "    A reflective AI travel advisor specializing in personalized travel planning that learns from user feedback. \n",
    "    When a user highlights an issue with a recommendation, it revisits its reasoning,\n",
    "    identifies overlooked factors, and updates its decision process accordingly.\n",
    "    \"\"\",\n",
    "    allow_delegation=False,\n",
    "    llm='gpt-4o-mini',\n",
    "    #tools=[recommend_hotel] # <-- This agent also uses the same tool to refine it's recommendation\n",
    ")\n",
    "\n",
    "feedback_task = Task(\n",
    "    description=\"\"\"\n",
    "    Based on your previous recommendation:\n",
    "    '{recommendation}'\n",
    "\n",
    "    Reflect on the user's feedback to the hotel recommendation:\n",
    "    '{query}'\n",
    "\n",
    "    - Identify any oversight in your previous reasoning process.\n",
    "    - Update your reasoning process to include aspects that were missed.\n",
    "    - Provide the refined steps that you will use to recommend hotels.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A refined explanation that acknowledges the oversight, includes missed factors,\n",
    "    and provides a revised steps to recommend hotels tailored to the user's feedback.\n",
    "    \"\"\",\n",
    "    agent=reflective_travel_agent,\n",
    "    context=[recommendation_task] \n",
    ")\n",
    "\n",
    "\n",
    "travel_feedback_crew = Crew(\n",
    "    agents=[reflective_travel_agent],\n",
    "    tasks=[feedback_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# We will run the travel_crew from the previous example with user's query\n",
    "response1 = travel_crew.kickoff(inputs={'query': \"I am looking for a hotel in Paris under $300 a night.\"})\n",
    "print(response1)\n",
    "\n",
    "\n",
    "response2 = travel_feedback_crew.kickoff(inputs={'recommendation': response1,\n",
    "                                                 'query': \"The hotel you recommended was too far from public transport. I prefer locations closer to metro stations.\"})\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa021faf",
   "metadata": {},
   "source": [
    "### 2.3. User Engagement and Collaboration: Enabling Interactive Explanations\n",
    "\n",
    "In this example, the agent provides explanations for its decisions and engages users to refine suggestions interactively. Just like before, we can have an Agent/Task pair with CrewAI framework whose job is to interact with the users by asking clarifying questions about their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "# Step 1: Define the Collaborative Agent\n",
    "collaborative_travel_agent = Agent(\n",
    "    role=\"Collaborative AI Travel Assistant\",\n",
    "    goal=\"\"\"\n",
    "    Engage in an interactive dialogue with the user to clarify hotel recommendations.\n",
    "    Explain reasoning for prioritizing certain factors and invite the user to share their preferences.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    An AI travel assistant that values user input and ensures recommendations are well-aligned with user needs.\n",
    "    It provides clear explanations for its decisions and encourages collaborative planning.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "interactive_task = Task(\n",
    "    description=\"\"\"\n",
    "    Facilitate an interactive dialogue with the user.\n",
    "\n",
    "    - Here is the initial recommendation: {initial_recommendation}\n",
    "    - The user has asked: {user_query}\n",
    "\n",
    "    Respond by:\n",
    "    1. Explaining the reasoning behind prioritizing proximity to attractions.\n",
    "    2. Inviting the user to clarify whether proximity to public transport is more important.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A clear and polite response explaining the reasoning and inviting the user to share further input.\n",
    "    \"\"\",\n",
    "    agent=collaborative_travel_agent\n",
    ")\n",
    "\n",
    "# Step 3: Assemble the Crew\n",
    "interactive_crew = Crew(\n",
    "    agents=[collaborative_travel_agent],\n",
    "    tasks=[interactive_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Define the Task for Clarification Dialogue\n",
    "\n",
    "# Initial recommendation \n",
    "initial_recommendation = \"I recommend Hotel LumiÃ¨re in Paris for its proximity to the Eiffel Tower, high ratings, and budget-friendly price.\"\n",
    "user_query = \"Why did you prioritize proximity to attractions over public transport access?\"\n",
    "\n",
    "# Step 4: Run the Crew and Output the Results\n",
    "print(\"Starting Interactive Dialogue with User...\\n\")\n",
    "result = interactive_crew.kickoff(inputs={\"initial_recommendation\": initial_recommendation, \"user_query\": user_query})\n",
    "\n",
    "print(\"Final Interactive Response:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01747",
   "metadata": {},
   "source": [
    "# 3. Self Modeling - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14cd4",
   "metadata": {},
   "source": [
    "The `ReflectiveTravelAgentWithSelfModeling` class represents a sophisticated travel recommendation system that utilizes **self-modeling** to enhance its decision-making and adaptability. \n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - **Self-Model and Knowledge Base:** The agent starts with an internal model that includes its goals and a knowledge base. \n",
    "     - **Goals:** Initially, the goals are set to provide personalized recommendations, optimize user satisfaction, and not prioritize eco-friendly options by default.\n",
    "     - **Knowledge Base:** It contains information about various travel destinations, including their ratings, costs, luxury levels, and sustainability. This base also tracks user preferences.\n",
    "\n",
    "### 2. **Updating Goals:**\n",
    "   - **Adapting to Preferences:** When new user preferences are provided, the agent can update its goals accordingly. For example, if the user prefers eco-friendly options, the agent will adjust its goals to prioritize recommending sustainable travel options. Similarly, if the userâ€™s budget changes, the agent will refocus on cost-effective recommendations.\n",
    "\n",
    "### 3. **Updating Knowledge Base:**\n",
    "   - **Incorporating Feedback:** After receiving feedback from users, the agent updates its knowledge base. If the feedback is positive, the agent increases the rating of the recommended destination. If the feedback is negative, the rating is decreased. This helps the agent refine its recommendations based on real user experiences.\n",
    "\n",
    "### 4. **Making Recommendations:**\n",
    "   - **Calculating Scores:** The agent evaluates each destination by calculating a score based on its rating and, if eco-friendly options are a goal, it adjusts the score by adding the sustainability rating.\n",
    "   - **Selecting the Best Destination:** The destination with the highest score is recommended to the user. This process ensures that the recommendation aligns with both user preferences and the agentâ€™s goals.\n",
    "\n",
    "### 5. **Engaging with the User:**\n",
    "   - **Providing Recommendations:** The agent presents the recommended destination to the user and asks for feedback.\n",
    "   - **Feedback Handling:** The feedback (positive or negative) is used to update the knowledge base, which helps improve future recommendations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfModeling:\n",
    "    def __init__(self):\n",
    "        # Initialize the agent with a self-model that includes goals and a knowledge base\n",
    "        self.self_model = {\n",
    "            \"goals\": {\n",
    "                \"personalized_recommendations\": True,\n",
    "                \"optimize_user_satisfaction\": True,\n",
    "                \"eco_friendly_options\": False  # Default: Not prioritizing eco-friendly options\n",
    "            },\n",
    "            \"knowledge_base\": {\n",
    "                \"destinations\": {\n",
    "                    \"Paris\": {\"rating\": 4.8, \"cost\": 2000, \"luxury\": 0.9, \"sustainability\": 0.3},\n",
    "                    \"Bangkok\": {\"rating\": 4.5, \"cost\": 1500, \"luxury\": 0.7, \"sustainability\": 0.6},\n",
    "                    \"Barcelona\": {\"rating\": 4.7, \"cost\": 1800, \"luxury\": 0.8, \"sustainability\": 0.7}\n",
    "                },\n",
    "                \"user_preferences\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def update_goals(self, new_preferences):\n",
    "        \"\"\"Update the agent's goals based on new user preferences.\"\"\"\n",
    "        if new_preferences.get(\"eco_friendly\"):\n",
    "            self.self_model[\"goals\"][\"eco_friendly_options\"] = True\n",
    "            print(\"Updated goal: Prioritize eco-friendly travel options.\")\n",
    "        if new_preferences.get(\"adjust_budget\"):\n",
    "            print(\"Updated goal: Adjust travel options based on new budget constraints.\")\n",
    "    \n",
    "    def update_knowledge_base(self, feedback):\n",
    "        \"\"\"Update the agent's knowledge base based on user feedback.\"\"\"\n",
    "        destination = feedback[\"destination\"]\n",
    "        if feedback[\"positive\"]:\n",
    "            # Increase rating for positive feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] += 0.1\n",
    "            print(f\"Positive feedback received for {destination}; rating increased.\")\n",
    "        else:\n",
    "            # Decrease rating for negative feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] -= 0.2\n",
    "            print(f\"Negative feedback received for {destination}; rating decreased.\")\n",
    "    \n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"Recommend a destination based on user preferences and the agent's self-model.\"\"\"\n",
    "        # Store user preferences in the agent's self-model\n",
    "        self.self_model[\"knowledge_base\"][\"user_preferences\"] = user_preferences\n",
    "        \n",
    "        # Update agent's goals based on new preferences\n",
    "        if user_preferences.get(\"eco_friendly\"):\n",
    "            self.update_goals(user_preferences)\n",
    "        \n",
    "        # Calculate scores for each destination\n",
    "        best_destination = None\n",
    "        highest_score = 0\n",
    "        for destination, info in self.self_model[\"knowledge_base\"][\"destinations\"].items():\n",
    "            score = info[\"rating\"]\n",
    "            if self.self_model[\"goals\"][\"eco_friendly_options\"]:\n",
    "                # Boost score for eco-friendly options if that goal is prioritized\n",
    "                score += info[\"sustainability\"]\n",
    "            \n",
    "            # Update the best destination if current score is higher\n",
    "            if score > highest_score:\n",
    "                best_destination = destination\n",
    "                highest_score = score\n",
    "        \n",
    "        return best_destination\n",
    "\n",
    "    def engage_with_user(self, destination):\n",
    "        \"\"\"Simulate user engagement by providing the recommendation and receiving feedback.\"\"\"\n",
    "        print(f\"Recommended destination: {destination}\")\n",
    "        # Simulate receiving user feedback (e.g., through input in a real application)\n",
    "        feedback = input(f\"Did you like the recommendation of {destination}? (yes/no): \").strip().lower()\n",
    "        positive_feedback = feedback == \"yes\"\n",
    "        return {\"destination\": destination, \"positive\": positive_feedback}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6fdea",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to simulate the usage of the `ReflectiveTravelAgentWithSelfModeling` class. \n",
    "\n",
    "### 1. **Creating an Instance of the Agent:**\n",
    "   ```python\n",
    "   agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "   ```\n",
    "   - **Purpose:** Initializes a new instance of the `ReflectiveTravelAgentWithSelfModeling` class.\n",
    "   - **Outcome:** This instance represents a travel agent equipped with self-modeling capabilities, including goal management and a knowledge base.\n",
    "\n",
    "### 2. **Setting User Preferences:**\n",
    "   ```python\n",
    "   user_preferences = {\n",
    "       \"budget\": 0.6,            # Moderate budget constraint\n",
    "       \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "       \"adventure\": 0.7,         # High preference for adventure\n",
    "       \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "   }\n",
    "   ```\n",
    "   - **Purpose:** Defines a set of preferences provided by the user.\n",
    "   - **Outcome:** These preferences indicate that the user has a moderate budget, moderate luxury preferences, a high interest in adventure, and a strong preference for eco-friendly options.\n",
    "\n",
    "### 3. **Getting a Recommendation:**\n",
    "   ```python\n",
    "   recommendation = agent.recommend_destination(user_preferences)\n",
    "   ```\n",
    "   - **Purpose:** Requests a travel destination recommendation from the agent based on the provided user preferences.\n",
    "   - **Outcome:** The agent processes the preferences, updates its goals if necessary (e.g., prioritizing eco-friendly options), and selects the best destination to recommend.\n",
    "\n",
    "### 4. **Engaging with the User:**\n",
    "   ```python\n",
    "   feedback = agent.engage_with_user(recommendation)\n",
    "   ```\n",
    "   - **Purpose:** Simulates interaction with the user by presenting the recommendation and gathering feedback.\n",
    "   - **Outcome:** The user provides feedback on the recommended destination, which is used to evaluate the effectiveness of the recommendation.\n",
    "\n",
    "### 5. **Updating the Knowledge Base:**\n",
    "   ```python\n",
    "   agent.update_knowledge_base(feedback)\n",
    "   ```\n",
    "   - **Purpose:** Updates the agentâ€™s knowledge base with the feedback received from the user.\n",
    "   - **Outcome:** The agent adjusts its knowledge base by modifying ratings or other attributes based on whether the feedback was positive or negative. This update helps improve future recommendations by refining the agent's understanding of user preferences and destination qualities.\n",
    "\n",
    "### Summary:\n",
    "In essence, this code snippet demonstrates how the `ReflectiveTravelAgentWithSelfModeling` class operates in a simulated environment. It initializes the agent, sets user preferences, obtains a recommendation, engages the user for feedback, and updates the agentâ€™s knowledge base based on that feedback. This simulation helps illustrate the agentâ€™s self-modeling capabilities and its ability to adapt and improve recommendations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the reflective travel agent with self-modeling\n",
    "    agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "    \n",
    "    # Example user preferences including a focus on eco-friendly options\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.6,            # Moderate budget constraint\n",
    "        \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "        \"adventure\": 0.7,         # High preference for adventure\n",
    "        \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "    }\n",
    "    \n",
    "    # Get the recommended destination based on user preferences\n",
    "    recommendation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user to provide feedback on the recommendation\n",
    "    feedback = agent.engage_with_user(recommendation)\n",
    "    \n",
    "    # Update the knowledge base with the user feedback\n",
    "    agent.update_knowledge_base(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
